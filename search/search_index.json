{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"selamat datang di halaman tugas penambangan data \u00b6 profil \u00b6 `NAMA : MOH. FARHAN HAMID `NIM :180411100132 `KELAS :PENAMBANGAN DATA 5-D `JURUSAN : TEKNIK INFORMATIKA \u200b","title":"index"},{"location":"#selamat-datang-di-halaman-tugas-penambangan-data","text":"","title":"selamat datang di halaman tugas penambangan data"},{"location":"#profil","text":"`NAMA : MOH. FARHAN HAMID `NIM :180411100132 `KELAS :PENAMBANGAN DATA 5-D `JURUSAN : TEKNIK INFORMATIKA \u200b","title":"profil"},{"location":"DECISION TREE/","text":"DECISION TREE \u00b6 decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan. CARA MEMBUAT DECISION TREE \u00b6 \u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"DECISION TREE"},{"location":"DECISION TREE/#decision-tree","text":"decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan.","title":"DECISION TREE"},{"location":"DECISION TREE/#cara-membuat-decision-tree","text":"\u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"CARA MEMBUAT DECISION TREE"},{"location":"Statistika Data/","text":"Statistika Data \u00b6 1. Rata-Rata \u00b6 Rata-rata atau disebut sebagai mean adalah suatu nilai hasil dari membagi jumlah nilai data dengan banyaknya data. Rata-rata menunjukkan pusat dari nilai data dan dapat mewakili dari keterpusatan data. Rata-rata merupakan suatu ukuran untuk memberikan gambaran yang lebih jelas dan singkat tentang sekumpulan data mengenai suatu persoalan. Rumusnya adalah: $$ \\begin{align} \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i \\end{align} $$ Bentuk programnya adalah: tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] jumlah = len ( tinggi ) total = 0 for angka in tinggi : total = total + angka mean = total / jumlah print ( \"Total nilai data adalah:\" , total ) print ( \"Rata-rata dari data adalah:\" , \"{:.2f}\" . format ( mean )) 2. Modus \u00b6 Modus adalah nilai yang mempunyai frekuensi terbesar dalam suatu kumpulan data. Modus berguna untuk mengetahui tingkat keseringan terjadinya suatu peristiwa. 3. Nilai Maksimum dan Minimum \u00b6 Nilai maksimum dan minimum digunakan untuk mencari nilai terbesar dan terkecil dari sebuah data. Program untuk mencari nilai maksimum dari data adalah: def maximum ( tinggi ): max_ = tinggi [ 0 ] for item in tinggi : if item > max_ : max_ = item return max_ tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] print ( \"Nilai maksimum dari data tersebut adalah:\" , maximum ( tinggi )) Program untuk mencari nilai minimum dari data adalah: def minimum ( tinggi ): min_ = tinggi [ 0 ] for item in tinggi : if item < min_ : min_ = item return min_ tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] print ( \"Nilai minimum dari data tersebut adalah:\" , minimum ( tinggi )) 4. Median \u00b6 Median adalah nilai tengah dari data yang telah disusun berurutan mulai dari yang terkecil sampai dengan yang terbesar. def median ( lst ): sortedLst = sorted ( lst ) panjang_list = len ( lst ) index = ( panjang_list - 1 ) // 2 if ( panjang_list % 2 ): return sortedLst [ index ] else : return ( sortedLst [ index ] + sortedLst [ index + 1 ]) / 2.0 tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] print ( \"Median data ini adalah:\" , median ( tinggi )) 5. Kuartil \u00b6 Kuartil adalah Q1, Q2, dan Q3 yang membagi nilai-nilai pada data menjadi empat bagian sama rata. - Q1 atau kuartil pertama adalah membatasi 25 persen data. - Q2 atau kuartil kedua adalah membatasi 50 persen data. Kuartil kedua dapat juga disebut sebagai median atau nilai tengah data. - Q3 atau kuartil ketiga adalah membatasi 75 persen data. tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] tinggi . sort () q1 = tinggi [ int ( len ( tinggi ) * 1 / 4 )] q2 = tinggi [ int ( len ( tinggi ) * 2 / 4 )] q3 = tinggi [ int ( len ( tinggi ) * 3 / 4 )] q4 = tinggi [ len ( tinggi ) - 1 ] print ( \"Datanya adalah:\" , tinggi ) print ( \"Data di kuartil 1 (Q1) adalah:\" , q1 ) print ( \"Data di kuartil 2 (Q2) adalah:\" , q2 ) print ( \"Data di kuartil 3 (Q3) adalah:\" , q3 ) 6. Standar Deviasi \u00b6 Standar deviasi adalah nilai statistik yang dimanfaatkan untuk menentukan bagaimana sebaran data dalam sampel, serta seberapa dekat titik data individu ke mean atau rata-rata nilai sampel. Sebuah standar deviasi dari kumpulan data sama dengan nol menandakan bahwa semua nilai dalam himpunan tersebut adalah sama. Sedangkan nilai deviasi yang lebih besar menunjukkan bahwa titik data individu jauh dari nilai rata-rata. Rumus mencari standar deviasi adalah: $$ \\begin{align} \\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2} \\end{align} $$ Contoh programnya adalah: tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] #Inisialisasi variabel jumlah_nilai = 0 jumlah_data = len ( tinggi ) #Menghitung jumlah nilai data for angka in tinggi : jumlah_nilai += angka #Menghitung rata-rata data mean = jumlah_nilai / jumlah_data #Inisialisasi variabel standard_dev dengan nilai 0 standard_dev = 0 #Menghitung nilai untuk mencari standar deviasi for elemen in tinggi : standard_dev += ( float ( elemen ) - mean ) ** 2 #Menghitung nilai akhir standar deviasi standard_dev = ( standard_dev * 1 / jumlah_data ) ** ( 1 / 2 ) print ( \"Standar deviasinya adalah:\" , \"{:.2f}\" . format ( standard_dev )) 7. Kemencengan ( Skewness ) \u00b6 Ukuran kemiringan adalah ukuran yang menyatakan derajat ketidaksimetrisan suatu lengkungan halus (kurva) dari suatu distribusi frekuensi. Kemiringan distribusi data ada tiga jenis, yaitu simetris, menceng ke kanan yang berarti kemiringannya positif, dan menceng ke kiri yang berarti kemiringannya negatif. Rumus mencari kemencengan adalah: $$ sk = \\frac {\\bar{X} - Mo}{s} $$ Contoh Program untuk Sebuah Data \u00b6 Program ini menggunakan modul Pandas dan Stats, untuk membuat tampilan tabel menggunakan HTML, display, dan tabulate. Data yang digunakan adalah: import pandas as pd from scipy import stats df = pd . read_csv ( \"data.csv\" ) from IPython.display import HTML , display import tabulate table = [ [ \"method\" ] + [ x for x in df . columns ], [ \"describe()\" ] + [ \"<pre>\" + str ( df [ col ] . describe ()) + \"</pre>\" for col in df . columns ], [ \"count()\" ] + [ df [ col ] . count () for col in df . columns ], [ \"mean()\" ] + [ df [ col ] . mean () for col in df . columns ], [ \"std()\" ] + [ \"{:.2f}\" . format ( df [ col ] . std ()) for col in df . columns ], [ \"max()\" ] + [ df [ col ] . max () for col in df . columns ], [ \"min()\" ] + [ df [ col ] . min () for col in df . columns ], [ \"q1()\" ] + [ df [ col ] . quantile ( 0.25 ) for col in df . columns ], [ \"q2()\" ] + [ df [ col ] . quantile ( 0.50 ) for col in df . columns ], [ \"q3()\" ] + [ df [ col ] . quantile ( 0.75 ) for col in df . columns ], [ \"skew()\" ] + [ \"{:.2f}\" . format ( df [ col ] . skew ()) for col in df . columns ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = \"html\" )))","title":"Statistik Deskriptif"},{"location":"Statistika Data/#statistika-data","text":"","title":"Statistika Data"},{"location":"Statistika Data/#1-rata-rata","text":"Rata-rata atau disebut sebagai mean adalah suatu nilai hasil dari membagi jumlah nilai data dengan banyaknya data. Rata-rata menunjukkan pusat dari nilai data dan dapat mewakili dari keterpusatan data. Rata-rata merupakan suatu ukuran untuk memberikan gambaran yang lebih jelas dan singkat tentang sekumpulan data mengenai suatu persoalan. Rumusnya adalah: $$ \\begin{align} \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i \\end{align} $$ Bentuk programnya adalah: tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] jumlah = len ( tinggi ) total = 0 for angka in tinggi : total = total + angka mean = total / jumlah print ( \"Total nilai data adalah:\" , total ) print ( \"Rata-rata dari data adalah:\" , \"{:.2f}\" . format ( mean ))","title":"1. Rata-Rata"},{"location":"Statistika Data/#2-modus","text":"Modus adalah nilai yang mempunyai frekuensi terbesar dalam suatu kumpulan data. Modus berguna untuk mengetahui tingkat keseringan terjadinya suatu peristiwa.","title":"2. Modus"},{"location":"Statistika Data/#3-nilai-maksimum-dan-minimum","text":"Nilai maksimum dan minimum digunakan untuk mencari nilai terbesar dan terkecil dari sebuah data. Program untuk mencari nilai maksimum dari data adalah: def maximum ( tinggi ): max_ = tinggi [ 0 ] for item in tinggi : if item > max_ : max_ = item return max_ tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] print ( \"Nilai maksimum dari data tersebut adalah:\" , maximum ( tinggi )) Program untuk mencari nilai minimum dari data adalah: def minimum ( tinggi ): min_ = tinggi [ 0 ] for item in tinggi : if item < min_ : min_ = item return min_ tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] print ( \"Nilai minimum dari data tersebut adalah:\" , minimum ( tinggi ))","title":"3. Nilai Maksimum dan Minimum"},{"location":"Statistika Data/#4-median","text":"Median adalah nilai tengah dari data yang telah disusun berurutan mulai dari yang terkecil sampai dengan yang terbesar. def median ( lst ): sortedLst = sorted ( lst ) panjang_list = len ( lst ) index = ( panjang_list - 1 ) // 2 if ( panjang_list % 2 ): return sortedLst [ index ] else : return ( sortedLst [ index ] + sortedLst [ index + 1 ]) / 2.0 tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] print ( \"Median data ini adalah:\" , median ( tinggi ))","title":"4. Median"},{"location":"Statistika Data/#5-kuartil","text":"Kuartil adalah Q1, Q2, dan Q3 yang membagi nilai-nilai pada data menjadi empat bagian sama rata. - Q1 atau kuartil pertama adalah membatasi 25 persen data. - Q2 atau kuartil kedua adalah membatasi 50 persen data. Kuartil kedua dapat juga disebut sebagai median atau nilai tengah data. - Q3 atau kuartil ketiga adalah membatasi 75 persen data. tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] tinggi . sort () q1 = tinggi [ int ( len ( tinggi ) * 1 / 4 )] q2 = tinggi [ int ( len ( tinggi ) * 2 / 4 )] q3 = tinggi [ int ( len ( tinggi ) * 3 / 4 )] q4 = tinggi [ len ( tinggi ) - 1 ] print ( \"Datanya adalah:\" , tinggi ) print ( \"Data di kuartil 1 (Q1) adalah:\" , q1 ) print ( \"Data di kuartil 2 (Q2) adalah:\" , q2 ) print ( \"Data di kuartil 3 (Q3) adalah:\" , q3 )","title":"5. Kuartil"},{"location":"Statistika Data/#6-standar-deviasi","text":"Standar deviasi adalah nilai statistik yang dimanfaatkan untuk menentukan bagaimana sebaran data dalam sampel, serta seberapa dekat titik data individu ke mean atau rata-rata nilai sampel. Sebuah standar deviasi dari kumpulan data sama dengan nol menandakan bahwa semua nilai dalam himpunan tersebut adalah sama. Sedangkan nilai deviasi yang lebih besar menunjukkan bahwa titik data individu jauh dari nilai rata-rata. Rumus mencari standar deviasi adalah: $$ \\begin{align} \\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2} \\end{align} $$ Contoh programnya adalah: tinggi = [ 173 , 166 , 161 , 167 , 166 , 163 , 176 , 162 , 180 , 163 , 167 , 179 , 168 , 165 , 177 ] #Inisialisasi variabel jumlah_nilai = 0 jumlah_data = len ( tinggi ) #Menghitung jumlah nilai data for angka in tinggi : jumlah_nilai += angka #Menghitung rata-rata data mean = jumlah_nilai / jumlah_data #Inisialisasi variabel standard_dev dengan nilai 0 standard_dev = 0 #Menghitung nilai untuk mencari standar deviasi for elemen in tinggi : standard_dev += ( float ( elemen ) - mean ) ** 2 #Menghitung nilai akhir standar deviasi standard_dev = ( standard_dev * 1 / jumlah_data ) ** ( 1 / 2 ) print ( \"Standar deviasinya adalah:\" , \"{:.2f}\" . format ( standard_dev ))","title":"6. Standar Deviasi"},{"location":"Statistika Data/#7-kemencengan-skewness","text":"Ukuran kemiringan adalah ukuran yang menyatakan derajat ketidaksimetrisan suatu lengkungan halus (kurva) dari suatu distribusi frekuensi. Kemiringan distribusi data ada tiga jenis, yaitu simetris, menceng ke kanan yang berarti kemiringannya positif, dan menceng ke kiri yang berarti kemiringannya negatif. Rumus mencari kemencengan adalah: $$ sk = \\frac {\\bar{X} - Mo}{s} $$","title":"7. Kemencengan (Skewness)"},{"location":"Statistika Data/#contoh-program-untuk-sebuah-data","text":"Program ini menggunakan modul Pandas dan Stats, untuk membuat tampilan tabel menggunakan HTML, display, dan tabulate. Data yang digunakan adalah: import pandas as pd from scipy import stats df = pd . read_csv ( \"data.csv\" ) from IPython.display import HTML , display import tabulate table = [ [ \"method\" ] + [ x for x in df . columns ], [ \"describe()\" ] + [ \"<pre>\" + str ( df [ col ] . describe ()) + \"</pre>\" for col in df . columns ], [ \"count()\" ] + [ df [ col ] . count () for col in df . columns ], [ \"mean()\" ] + [ df [ col ] . mean () for col in df . columns ], [ \"std()\" ] + [ \"{:.2f}\" . format ( df [ col ] . std ()) for col in df . columns ], [ \"max()\" ] + [ df [ col ] . max () for col in df . columns ], [ \"min()\" ] + [ df [ col ] . min () for col in df . columns ], [ \"q1()\" ] + [ df [ col ] . quantile ( 0.25 ) for col in df . columns ], [ \"q2()\" ] + [ df [ col ] . quantile ( 0.50 ) for col in df . columns ], [ \"q3()\" ] + [ df [ col ] . quantile ( 0.75 ) for col in df . columns ], [ \"skew()\" ] + [ \"{:.2f}\" . format ( df [ col ] . skew ()) for col in df . columns ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = \"html\" )))","title":"Contoh Program untuk Sebuah Data"},{"location":"Tugas 2/","text":"Mengukur Jarak Data \u00b6 Mengukur Jarak Tipe Numerik \u00b6 Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya Minkowski Distance \u00b6 \u00b6 Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan dimana m adalah bilangan riel positif dan x i dan y i adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. Manhattan distance \u00b6 \u00b6 Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan Euclidean distance \u00b6 \u00b6 Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance \u00b6 \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan Weighted euclidean distance \u00b6 \u00b6 Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan dimana wi adalah bobot yang diberikan pada atribut ke i. Chord distance \u00b6 \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan dimana \u2016 x \u20162 adalah L 2-norm . Mahalanobis distance \u00b6 \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan dimana S adalah matrik covariance data. Cosine measure \u00b6 \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn) didefinisikan dengan Pearson correlation \u00b6 \u00b6 Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan , dimana \u03bc x The Pearson correlation kelemahannya adalah sensitif terhadap outlier Mengukur Jarak Atribut Binary \u00b6 \u00b6 Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i dan j, r adalah jumlah atribut yang sama dengan 1 untuk objek i tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, di mana p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i dan j dapat dihitung dengan Persamaan similarity ini disebut dengan Jaccard coefficient Mengukur Jarak Tipe categorical \u00b6 \u00b6 Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269. Overlay Metric \u00b6 \u00b6 Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan dimana nn adalah banyaknya atribut, ai(x) dan ai(y) adalah nilai atribut ke i yaitu Ai dari masing masing objek x dan y, \u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi. Value Difference Metric (VDM) \u00b6 \u00b6 VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan dimana C adalah banyaknya kelas, P(c|ai(x)) adalah probabilitas bersyarat dimana kelas x adalah c dari atribut Ai, yang memiliki nilai ai(x), P(c|ai(y)) adalah probabilitas bersyarat dimana kelas y adalah c dengan atribut Ai memiliki nilai ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes, Minimum Risk Metric (MRM) \u00b6 \u00b6 Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan Mengukur Jarak Tipe Ordinal \u00b6 \u00b6 Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f yang memiliki Mf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f untuk objek ke-i adalah xif, dan ff memiliki Mf status urutan , mewakili peringkat 1,..,Mf Ganti setiap xif dengan peringkatnya, rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rif dengan Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$ Menghitung Jarak Tipe Campuran \u00b6 \u00b6 Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0][0,0,1.0]. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan dimana =0 - jika xif atau xjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek i atau objek j) jika xif=xjf=0 dan atribut f adalah binary asymmetric, selain itu =1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu. ) dihitung bergantung pada tipenya, Jika ff adalah numerik, , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjf, sebaliknya =1 Jika ff adalah ordinal maka hitung rangking dan perlakukan zif sebagai numerik. Contoh Program \u00b6 import pandas as pd import itertools as it import scipy.spatial.distance as spad df = pd . read_csv ( 'data.csv' , nrows = 10 ) data = [[ round ( i , 2 ) for i in x ] for x in df . values . tolist ()] df = pd . DataFrame ( data , columns = df . columns ) df import pandas as pd import itertools as it import scipy.spatial.distance as spad columns = [ \"Binary\" , \"Course Instructor\" , \"M=1\" , \"M=2\" ] distdata = list ( it . combinations ( data , 2 )) data2 = [( d [ 0 ][ 1 ], d [ 1 ][ 1 ], spad . cityblock ( d [ 0 ][ 1 :], d [ 1 ][ 1 :]), spad . euclidean ( d [ 0 ][ 1 :], d [ 1 ][ 1 :])) for d in distdata ] pd . DataFrame ( data2 , columns = columns )","title":"Jarak"},{"location":"Tugas 2/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"Tugas 2/#mengukur-jarak-tipe-numerik","text":"Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya","title":"Mengukur Jarak Tipe Numerik"},{"location":"Tugas 2/#minkowski-distance","text":"Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan dimana m adalah bilangan riel positif dan x i dan y i adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.","title":"Minkowski Distance\u00b6"},{"location":"Tugas 2/#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan","title":"Manhattan distance\u00b6"},{"location":"Tugas 2/#euclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Euclidean distance\u00b6"},{"location":"Tugas 2/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan","title":"Average Distance\u00b6"},{"location":"Tugas 2/#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan dimana wi adalah bobot yang diberikan pada atribut ke i.","title":"Weighted euclidean distance\u00b6"},{"location":"Tugas 2/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan dimana \u2016 x \u20162 adalah L 2-norm .","title":"Chord distance\u00b6"},{"location":"Tugas 2/#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan dimana S adalah matrik covariance data.","title":"Mahalanobis distance\u00b6"},{"location":"Tugas 2/#cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn) didefinisikan dengan","title":"Cosine measure\u00b6"},{"location":"Tugas 2/#pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan , dimana \u03bc x The Pearson correlation kelemahannya adalah sensitif terhadap outlier","title":"Pearson correlation\u00b6"},{"location":"Tugas 2/#mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i dan j, r adalah jumlah atribut yang sama dengan 1 untuk objek i tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, di mana p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i dan j dapat dihitung dengan Persamaan similarity ini disebut dengan Jaccard coefficient","title":"Mengukur Jarak Atribut Binary\u00b6"},{"location":"Tugas 2/#mengukur-jarak-tipe-categorical","text":"Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269.","title":"Mengukur Jarak Tipe categorical\u00b6"},{"location":"Tugas 2/#overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan dimana nn adalah banyaknya atribut, ai(x) dan ai(y) adalah nilai atribut ke i yaitu Ai dari masing masing objek x dan y, \u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.","title":"Overlay Metric\u00b6"},{"location":"Tugas 2/#value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan dimana C adalah banyaknya kelas, P(c|ai(x)) adalah probabilitas bersyarat dimana kelas x adalah c dari atribut Ai, yang memiliki nilai ai(x), P(c|ai(y)) adalah probabilitas bersyarat dimana kelas y adalah c dengan atribut Ai memiliki nilai ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes,","title":"Value Difference Metric (VDM)\u00b6"},{"location":"Tugas 2/#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan","title":"Minimum Risk Metric (MRM)\u00b6"},{"location":"Tugas 2/#mengukur-jarak-tipe-ordinal","text":"Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f yang memiliki Mf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f untuk objek ke-i adalah xif, dan ff memiliki Mf status urutan , mewakili peringkat 1,..,Mf Ganti setiap xif dengan peringkatnya, rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rif dengan Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$","title":"Mengukur Jarak Tipe Ordinal\u00b6"},{"location":"Tugas 2/#menghitung-jarak-tipe-campuran","text":"Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0][0,0,1.0]. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan dimana =0 - jika xif atau xjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek i atau objek j) jika xif=xjf=0 dan atribut f adalah binary asymmetric, selain itu =1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu. ) dihitung bergantung pada tipenya, Jika ff adalah numerik, , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjf, sebaliknya =1 Jika ff adalah ordinal maka hitung rangking dan perlakukan zif sebagai numerik.","title":"Menghitung Jarak Tipe Campuran\u00b6"},{"location":"Tugas 2/#contoh-program","text":"import pandas as pd import itertools as it import scipy.spatial.distance as spad df = pd . read_csv ( 'data.csv' , nrows = 10 ) data = [[ round ( i , 2 ) for i in x ] for x in df . values . tolist ()] df = pd . DataFrame ( data , columns = df . columns ) df import pandas as pd import itertools as it import scipy.spatial.distance as spad columns = [ \"Binary\" , \"Course Instructor\" , \"M=1\" , \"M=2\" ] distdata = list ( it . combinations ( data , 2 )) data2 = [( d [ 0 ][ 1 ], d [ 1 ][ 1 ], spad . cityblock ( d [ 0 ][ 1 :], d [ 1 ][ 1 :]), spad . euclidean ( d [ 0 ][ 1 :], d [ 1 ][ 1 :])) for d in distdata ] pd . DataFrame ( data2 , columns = columns )","title":"Contoh Program"},{"location":"Untitled8/","text":"MISSING VELLUES \u00b6 Data yang Hilang dapat terjadi ketika tidak ada informasi yang disediakan untuk satu atau lebih item atau untuk seluruh unit. Data yang Hilang adalah masalah yang sangat besar dalam skenario kehidupan nyata. Data yang Hilang juga bisa disebut sebagai nilai-nilai NA (Tidak Tersedia) di panda. Dalam DataFrame kadang-kadang banyak set data hanya datang dengan data yang hilang, baik karena itu ada dan tidak dikumpulkan atau tidak pernah ada. Sebagai Contoh, Misalkan pengguna lain yang disurvei mungkin memilih untuk tidak membagikan pendapatan mereka, beberapa pengguna mungkin memilih untuk tidak membagikan alamat dengan cara ini banyak set data hilang. Dalam Pandas data yang hilang diwakili oleh dua nilai: Tidak ada: Tidak ada adalah objek tunggal Python yang sering digunakan untuk data yang hilang dalam kode Python. NaN: NaN (akronim untuk Bukan Angka), adalah nilai titik-mengambang khusus yang dikenali oleh semua sistem yang menggunakan representasi titik-mengambang IEEE standar Panda memperlakukan None dan NaN sebagai dasarnya dapat dipertukarkan untuk menunjukkan nilai yang hilang atau nol. Untuk memfasilitasi konvensi ini, ada beberapa fungsi yang berguna untuk mendeteksi, menghapus, dan mengganti nilai null di Pandas DataFrame: isnull () notnull () dropna () fillna () menggantikan() menambah() # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score 0 100.0 30.0 0.0 1 90.0 45.0 40.0 2 0.0 56.0 80.0 3 95.0 0.0 98.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling a missing value with # previous ones df . fillna ( method = 'pad' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score 0 100.0 30.0 NaN 1 90.0 45.0 40.0 2 90.0 56.0 80.0 3 95.0 56.0 98.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling null value using fillna() function df . fillna ( method = 'bfill' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score 0 100.0 30.0 40.0 1 90.0 45.0 40.0 2 95.0 56.0 80.0 3 95.0 NaN 98.0","title":"MISSING VELUES"},{"location":"Untitled8/#missing-vellues","text":"Data yang Hilang dapat terjadi ketika tidak ada informasi yang disediakan untuk satu atau lebih item atau untuk seluruh unit. Data yang Hilang adalah masalah yang sangat besar dalam skenario kehidupan nyata. Data yang Hilang juga bisa disebut sebagai nilai-nilai NA (Tidak Tersedia) di panda. Dalam DataFrame kadang-kadang banyak set data hanya datang dengan data yang hilang, baik karena itu ada dan tidak dikumpulkan atau tidak pernah ada. Sebagai Contoh, Misalkan pengguna lain yang disurvei mungkin memilih untuk tidak membagikan pendapatan mereka, beberapa pengguna mungkin memilih untuk tidak membagikan alamat dengan cara ini banyak set data hilang. Dalam Pandas data yang hilang diwakili oleh dua nilai: Tidak ada: Tidak ada adalah objek tunggal Python yang sering digunakan untuk data yang hilang dalam kode Python. NaN: NaN (akronim untuk Bukan Angka), adalah nilai titik-mengambang khusus yang dikenali oleh semua sistem yang menggunakan representasi titik-mengambang IEEE standar Panda memperlakukan None dan NaN sebagai dasarnya dapat dipertukarkan untuk menunjukkan nilai yang hilang atau nol. Untuk memfasilitasi konvensi ini, ada beberapa fungsi yang berguna untuk mendeteksi, menghapus, dan mengganti nilai null di Pandas DataFrame: isnull () notnull () dropna () fillna () menggantikan() menambah() # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score 0 100.0 30.0 0.0 1 90.0 45.0 40.0 2 0.0 56.0 80.0 3 95.0 0.0 98.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling a missing value with # previous ones df . fillna ( method = 'pad' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score 0 100.0 30.0 NaN 1 90.0 45.0 40.0 2 90.0 56.0 80.0 3 95.0 56.0 98.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling null value using fillna() function df . fillna ( method = 'bfill' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score 0 100.0 30.0 40.0 1 90.0 45.0 40.0 2 95.0 56.0 80.0 3 95.0 NaN 98.0","title":"MISSING VELLUES"}]}